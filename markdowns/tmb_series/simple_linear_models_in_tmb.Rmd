---
title: "Simple Linear Models in TMB"
author: "Rebecca Stubbs"
date: "Thursday, December 08, 2016"
output: 
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
---

<style type="text/css">

body{ /* Normal  */
   font-size: 12px;
}
td {  /* Table  */
   font-size: 8px;
}
h1 { /* Header 1 */
 font-size: 35px;
 color: Black;
}
h2 { /* Header 2 */
 font-size: 20px;
 color: Black;
}
h3 { /* Header 3 */
 font-size: 16px;
 color: Black;
}
code.r{ /* Code block */
  font-size: 11px;
}
pre { /* Code block */
  font-size: 13px
}
</style>


The TMB library is a link that allows the high-level, statistically-oriented programming language R to interface with the computational speed of C++ for calculating likelihood. This example walks through how to generate simple parameter estimates for different distributions using R, then Template Model Builder.

This page details how to expand from discovering basic distribution parameters to creating basic, linear models that include covariates. 

Note: This is part 2 of an intro to TMB series-- if you're unfamiliar with TMB, you may want to check out part 1 here:
https://rpubs.com/BeccaStubbs/tmb_simple_distributions

# What is a linear predictor? 

Y=mx+b: the basic equation for linear models. What, functionally, does that mean within a framework that is designed to compare individual data points to distribution and modeling parameters?

In Part 1, we evaluated the relative likelihood that the data we observed was generated from a variety of different distributions, comparing the JNLL values that are generated by different mean and variance parameters. Fundamentally, this approach is trying to discover the _data generating process_- how did the data we are observing come to be? Using these distributions implies that we think there some sort of inherent order in the data we see-- that we believe there is some standard or common value, as well as variations or noise due to random chance (or observation/measurement error). 

Linear models are an extention of this same framework.

However, this time, we are bringing more prior knowledge to the table--we think that other variables we have observed might be able to predict the value or explain the variance in the data we are most interested in (our dependent variable-"y"). 

So, we again return to the _data generating process_-- how you think the patterns in your dependent variable (y) came to be-- what does the depenent variable depend on? These relationships could be hypothesized to be causal (x leads to y) or simply associative (x tells us something about y). Either way, we are making the assertions along the lines of "usually, when X changes, so does Y, by Z amount for each increment of X".

These relationships (X predicting Y) influence the distribution that we expect our data to have come from.

Just like how we simulated a data set before modeling it in part 1, we are going to build a data set from the ground up, where we "know" the relationships between our covariates and dependent variable.

As such, we prepare our workspace: load in our libraries, and define how many hypothetical samples we will be taking over the course of this experiment-- in this case, our theoretical sample size will be 1,000.
```{r,include=TRUE, message=FALSE, warning=F}
rm(list=ls())
library(data.table)
library(TMB)
library(ggplot2)
library(ggthemes)
set.seed(2016)
n_sampled<-1000
```

## What makes Blackberries sweet? Simulating a data set

Let's say your are an avid blackberry forager around the bike paths and canoe waterways of Seattle. After many years of picking blackberries, you get an intuition for what blackberry patches might be worth the thorns, and which ones are probably full of sour dissapointment. 

Let's simulate a blackberry population, with a sweetness rating. 

### Constructing a blackberry harvest

You think that the biggest predictor for whether a blackberry will be tasty or not is whether you picked it too soon. The blackberries that you picked have the following ages:

```{r, fig.height=2,fig.width=8}
blackberries<-data.table(days_ripening=15*rbeta(n_sampled,10,2))
ggplot(blackberries)+geom_density(aes(days_ripening))+labs(title="Number of days spent ripening")
```

You also have picked blackberries at a variety of elevations, so perhaps there also is a relationship between elevation and berry taste.

```{r, fig.height=2,fig.width=8}
blackberries[,elevation:=rnorm(n_sampled,1000,200)]
ggplot(blackberries)+geom_density(aes(elevation))+labs(title="Elevation of thicket, in feet")
```

In addition, you know that some berries have more time in the sun than others, and that the sun-ripened fruits are far more delicious. Maybe we picked 60% of our samples in sunny thickets, and 40% in shady spots. 

```{r, fig.height=2,fig.width=8}
blackberries<-cbind(blackberries,
                    rbind(data.table(sun_hours=rpois(n_sampled*.6,5),sun="Sun"),
                          data.table(sun_hours=rpois(n_sampled*.4,3),sun="Shade")))
ggplot(blackberries)+geom_density(aes(sun_hours,color=sun))+labs(title="Hours in the sun, by sunny or shady")
```

We also collected the distance from the blackberries to the nearest sidewalk. We'll add another column pulled from a uniform distribution to represent that variable as well:

```{r}
blackberries[,dist_to_sidewalk:=runif(n_sampled,0,30)]
```

To some extent, how sweet each individual berry is partially due to random chance-- so let's assume that the random component is normally distributed.

```{r, fig.height=2,fig.width=8}
blackberries[,random_component:=rnorm(n_sampled,0,7)]
ggplot(blackberries)+geom_density(aes(random_component))+labs(title="Random component")
```

### Understanding expected values

Let's say the following relationships occur between each of the variables and blackberry taste: 

* Intercept: In general, we expect there to be a baseline score regardless of any other factors, of 20 points on the tasty scale. Without knowing anything else about the fruits, we'd expect that we would often get tasty scores of around 20. 

* days ripening: For every day the blackberry ripens, the taste goes up 5 points.

* sun_hours: For each hour of sunlight exposure the berry has on an average day, we think it should get tastier-- maybe .5 points on the scale.

* elevation: We think the berries get worse when picked at higher elevations. As such, the higher we collect the blackberry, we think it will be just a little less flavorful- a taste score diniminshing 0.02 points per foot harvested above sealevel.

* dist to sidewalk: Turns out, this variable is totally irrelevant.

* random component: Either there's another driver of blackberry taste we haven't captured here (omission bias), we aren't very consistent with how we judge blackberry tastiness (measurement error), or it's truly random (stochastic process). In any case, there's some unexplained variation in berry taste. 

Our knowledge of each of these factors adjusts what we expect from a blackberry we have picked, and how surprised we are at the flavor we discover when we eat a blackberry picked under certain conditions. If you were picking blackberries under an overpass or in a forested ditch, you wouldn't be surprised if it was bitter-- if you picked one on a hillside in full sun in the middle of July, you would certainly hope that it would be delicious. 

Each one of those factors (elevation, days ripening, etc) *changes your expectation* as a berry-picker as to whether a particular thicket is likely to contain tasty fruit. If we combine all the things we know about any individual blackberry, with some factors increasing and some factors decreasing berry tastiness, we end up with one final 'expected value' or set of values that woudldn't be surprising given what we've observed about the berry's growing environment. Just as we have an "expectation" (as in a gut instinct, or preconcieved belief) for berry taste based on experience, we can have an expectation for a numerical value for berry tastiness-- this expectation is the mean of a distribution of possible berry flavors given the environment is was grown in.

Mathematically, the combination of each of the covariates with their beta coefficient values (the magnitude and direction of their relationship with the dependent variable) lead to the expected value for each data point. 

Knowing what we know about berry flavor, we can create a flavor score for our blackberries based on the relationships we defined above.

Here, we add a final column to the data set, "taste_score", which is the sum total of the following things: 

* The intercept (the base value of 20 flavor points)

* The impact of the covariates: the sum of each of the beta coefficients (the magnitude and direction of the relationship) multiplied by the covariate values we observed

* The impact of random chance: the contribution of error or noise

```{r, fig.height=3,fig.width=8}
blackberries[,taste_score:=20+(5*days_ripening)+(.5*sun_hours)+(-.02*elevation)+(0*dist_to_sidewalk)+random_component]

ggplot(blackberries)+
  geom_density(aes(taste_score))+
  labs(title="Distribution of blackberry tastiness")
```

Now, let's take a look at how that came out in terms of correlations between our dependent variable (taste) and the covariates: 

```{r, fig.height=3,fig.width=8}
ggplot(blackberries, aes(days_ripening, taste_score))+geom_point(aes(colour=sun))
ggplot(blackberries, aes(sun_hours, taste_score))+geom_point(aes(colour=sun))
ggplot(blackberries, aes(elevation, taste_score))+geom_point(aes(colour=sun))
```

## Modeling (in R):

Okay, so now say that we don't actually know ahead of time how these relationships between the covariates and the blackberry sweetness score play out. How would an optimizer figure out the magnitude and direction of each of those relationships? 

We still are comparing each data point to a set of distribution parameters (in this case, we are going to assume that our blackberry data are normally distributed, so they will have a mean and standard deviation parameter). However, the mean of the distribution we are comparing our data point to is generated by adding up all the information we know about the covariates that predict it. 

Let's return to the process we talked about in part 1, updated for this task:

* 0) Decide what distribution is appropriate (we have decided that the blackberries are distributed normally)

* 1) Come up with a starting guess for what the parameters are (for the standard deviation, and all of the beta coefficients, which will be combined with the covariate values to create each data point's specific mean)

* 2) Evaluate how likely it is that each point could have come from that distribution (evaluating each data point's likelihood of having been generated by the distribution that results from certain beta coefficients)

* 3) Tweak the parameters, and see whether it becomes more likely that the data was generated from that distribution

* 4) Repeat steps 2) and 3) until you reach a point of diminishing returns. 

```{r}

```

The optimizer would have to guess more parameters than just the mean and standard deviation this time-- it also would need to figure out the intecept, and the magnitude and direction of each of the beta coefficients. 

To fit model parameters, the optimizer tries out different beta coefficients (that describe the magnitude and direction of their relationship with the dependent variable), and compares the dependent variable to what we would expect based on what we know about its covariates. For each set of model parameters, the JNLL is calculated by evaluating the likelihood that each data point was generated by the that particular row's combination of beta coefficient and covariate values. The model is testing whether the data generation process described by that set of parameters seems plausible. 

In R, we could generate a function for calculating the 


