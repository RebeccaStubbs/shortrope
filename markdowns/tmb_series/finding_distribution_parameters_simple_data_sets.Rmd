---
title: 'Finding Distribution Parameters of Simple Data Sets in R and TMB'
author: "Rebecca Stubbs"
date: "Tuesday, December 06, 2016"
output: html_document
---


The TMB library is a link that allows the high-level, statistically-oriented programming language R to interface with the computational speed of C++ for calculating likelihood. This example walks through how to generate simple parameter estimates for different distributions using R, then Template Model Builder.

Unlike many of the function calls in R and regression and modeling packages available on CRAN, using Template Model Builder requires you to literally write out the equations to calculate your likelihood in order to run a model. As such, this page goes into the weeds of how to calculate joint negative log likelihood in R first (familiar territory), then switches to TMB in hopes that it will make the C++ versions more intelligible. 

So, let's start by getting a workspace set up: loading the required libraries, etc. 
```{r setup, include=TRUE, message=FALSE, warning=F}
rm(list=ls()) # Removing everything from the R workspace
library(TMB) # Template model builder
library(ggplot2) # Plotting package
library(data.table) #data manipulation package
library(ggthemes)
```

## Simulating some data

First, let's create 3 data sets, where we know ahead of time what the distribution parameters are. This has the advantage of allowing the programmer to know ahead of time what the "right answer" for parameters should be, which will allow us to check our code for errors.

```{r, include=TRUE}

simulated_data<-data.table(
                           
sim_norm=rnorm(1000,50,6), # Generating a column witihn the data set ("sim_norm") that has 1000 observations 
                          # pulled from a normal distribution of mean 50, with a standard deviation of 6

sim_pois=rpois(1000,10),  # Generating a column ("sim_poisson") that has 1000 observations 
                          # pulled from a poisson distribution with a lambda (expected count) of 10.

sim_beta=rbeta(1000,70,3)) # Generating a column ("sim_beta") that has 1000 observations 
                          # pulled from a beta distribution with parameters a=70, b=3
```

## Figuring out their parameters using R

Let's say for the sake of example that you *don't* already know the distribution parameters that were used to create these data sets, and you wanted to find out. To do this, we could roughly use the following process:

* 0) Decide what distribution is appropriate (for now, we will assume we are omnipotent and just "know")

* 1) Come up with a starting guess for what the parameters are

* 2) Evaluate how likely it is that each point could have come from that distribution

* 3) Tweak the parameters, and see whether it becomes more likley that the data was generated from that distribution

* 4) Repeat steps 2) and 3) until you reach a point of diminishing returns. 

Steps 2-4 are easy to complete with the help of computers and statistical packages through functions and optimization. 
Our intuition would tell us that data points closer to the mean of the distribution are probably more likely to have been generated by a process with those parameters than those far away from the mean. We also know that data points further from the mean are less surprising if the data is very noisy (has a high standard deviation). 

Let's use R's "dnorm" function to evaluate how likely it is that an individual point came from a distribution, and visualize that data point as a red line being portrayed over the distribution:

```{r, include=TRUE, fig.height=2, fig.width=7}
# Visualizing the data point over the distribution 
ggplot(data=simulated_data)+
  geom_density(aes(sim_norm))+
  geom_vline(xintercept=48,colour='red')+
  labs(title="A Likely Data Point (red)")+theme_tufte()

# Discovering the likelihood of the number 48 being 
# generated from a normal distribution with  
# mean 50, SD 6
dnorm(48,50,6)
```

Now, let's evaluate the likelihood of a data point that seems less likely to have come from that distribution:

```{r, include=TRUE, fig.height=2, fig.width=7}
ggplot(data=simulated_data)+
  geom_density(aes(sim_norm))+
  geom_vline(xintercept=75,colour='blue')+
  labs(title="A Less Likely Data Point (blue)")+theme_tufte()
dnorm(75,50,6)
```

As expected, the likelihood of the data point farther away from the mean is smaller. But that's just one data point-- what if you have a set of numbers that you think could have been generated from a given distribution?

## How low can you go: Optimizing the Joint Negative Log Likelihood

To evaluate the likelihood of the whole data set, we evaluate each data point's probability of having been generated from that distribution, and then combine the probabilities. When evaluating the probability of mulitple events, you need to multiply the probabilities together to get the likelihood that all of the events will occur. However, multiplication is computationally expensive as the numbers get very large-- so, instead of multiplying things together, we use the handy math trick of log-transforming the likelihood, which then allows the probabilities to be added together rather than multiplied. 

This creates a "joint log likelihood". This number would be positive-- however, computers like to optimize to find minimums, so instead, we create a "joint negative log likelihood" (JNLL) for an entire data set by starting off with a JNLL of 0, and then subtract each data point's likelihood from that number. The idea here is that an optimizer will be able to tweak the parameters of the distribution until it reaches the lowest JNLL that it can.

So, to calculate the JNLL of a data set given a set of parameters, you could do something like the following:

```{r}
# Defining a function to calculate the JNLL
calc_jnll<-function(the_dataset,mean_param,sd_param){
  jnll<-0 # Setting the Joint Negative Log likelihood to Zero -- throughout the loop,
          # this will get increasingly negative as each log-likleihood is subtracted from it
  for (each_datapoint in the_dataset){ # Creating a loop through each data point
    #Find the likelihood of that point
    #having log=TRUE means that it will return the log likelihood 
      likelihood_of_single_point<-dnorm(each_datapoint,mean_param,sd_param,log=TRUE) 
    # Subtract that from the jnll
      jnll<-jnll-likelihood_of_single_point
    }
  return(jnll)
  }
```

A far more efficient way of calculating this would be to pass the vector of data values, rather than a single data point, to the dnorm function:
```{r}
# Defining a more efficient function to calculate the JNLL
calc_jnll_fn<-function(data_vector,mean_param,sd_param){
  vector_of_likelihoods<-dnorm(data_vector,mean_param,sd_param,log=TRUE) #the dnorm() function can take a vector!
  jnll<- -1 * sum(vector_of_likelihoods) # now, we just make the 
  return(jnll)
}
```

Let's test out this function on three different guesses, and visualize the guesses compared to our data that we have "observed" (in this case, simulated).

```{r}
# Creating a data set based on the "real" (simulated) values, and the 
# creating data sets that would occur from distributions based on 2 parameter guesses
jnll_vis<-rbind(data.table(data=simulated_data$sim_norm, type="Simulated Data (mean=50,sd=6)"),
                data.table(data=rnorm(1000,40,10), type="Guess #1 (mean=40,sd=10)"),
                data.table(data=rnorm(1000,55,5), type="Guess #2 (mean=55,sd=5)"),
                data.table(data=rnorm(1000,50,6), type="Guess #3 (mean=50,sd=6)"))

ggplot(data=jnll_vis)+
  geom_density(aes(data,colour=type))+
  labs(title="Data vs. Parameter Guesses")
```

It looks like we would imagine that the third guess (which in truth, we are the parameters used to generate the siumulated data set), but let's check which guess has the lowest joint negative log-likelihood:

```{r}
# Guess #1
calc_jnll_fn(data_vector=simulated_data$sim_norm,mean_param=40,sd_param=10)

# Guess #2
calc_jnll_fn(data_vector=simulated_data$sim_norm,mean_param=55,sd_param=5)

# Guess #3
calc_jnll_fn(data_vector=simulated_data$sim_norm,mean_param=50,sd_param=6)
```

Looks like Guess # 3 (the true distribution parameters) wins out! But, that guess-and-check method is certainly far from ideal. Luckily, R has optimizers that can search through "parameter space" and find the combinations that achieve the lowest JNLL.  So, let's try using one of R's optimizers to find out what distribution our simulated data came from. 


Let's check out using optim, a function in the {stats} library in R, which is automatically loaded, to do our guesswork for us (and more ingelligently).

```{r}

# First, let's get a starting guess:
starting_guesses<-list("mean_param"=100,"sd_param"=0)


# Now, we can use the otim() function in R to try to find better parameters to describe the data. 
Opt <- optim(par=starting_guesses, fn=calc_jnll_fn, Data=simulated_data$sim_norm)

print(Opt$par) # Estimated parameters
#print(sqrt(diag(solve(Opt$hessian))))# This pulls the hessian matrix, "solves (aka. takes the inverse of it) it", 
# then takes the diagonal, and square roots it. 


```



